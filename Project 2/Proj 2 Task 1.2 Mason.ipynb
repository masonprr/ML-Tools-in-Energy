{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AtyRlclFkQAY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697511667261,"user_tz":420,"elapsed":5849,"user":{"displayName":"Mason Rodriguez Rand","userId":"15413783060171038202"}},"outputId":"772d7159-06fa-43af-8846-8b5a9b5f0730"},"outputs":[{"output_type":"stream","name":"stdout","text":["0    0.990099\n","1    0.990099\n","2    0.990099\n","3    1.000000\n","4    0.990099\n","5    1.000000\n","6    1.188119\n","7    1.782178\n","Name: x01, dtype: float64 0    0.896552\n","1    1.000000\n","2    1.055172\n","3    0.896552\n","4    1.000000\n","5    1.055172\n","6    0.896552\n","7    1.000000\n","Name: x02, dtype: float64 0    1.009091\n","1    1.000000\n","2    0.993506\n","3    1.009091\n","4    1.000000\n","5    0.993506\n","6    1.009091\n","7    1.000000\n","Name: x03, dtype: float64 0    0.956452\n","1    0.993796\n","2    0.978365\n","3    0.954292\n","4    0.999969\n","5    0.969106\n","6    1.096571\n","7    1.432055\n","Name: y3, dtype: float64\n","[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n","[[0.99009901 0.89655172 1.00909091]\n"," [0.99009901 1.         1.        ]\n"," [0.99009901 1.05517241 0.99350649]\n"," [1.         0.89655172 1.00909091]\n"," [0.99009901 1.         1.        ]\n"," [1.         1.05517241 0.99350649]\n"," [1.18811881 0.89655172 1.00909091]\n"," [1.78217822 1.         1.        ]]\n"]}],"source":["'''>>>>> start CodeP2.2F23-updated\n","    V.P. Carey ME249, Fall 2023\n","\n","Intro to Neural Network Modeling\n","Keras model for comparison with first principles model'''\n","\n","#import useful packages\n","import keras\n","import pandas as pd\n","from keras.models import Sequential\n","import numpy as np\n","import keras.backend as kb\n","import tensorflow as tf\n","#the follwoing 2 lines are only needed for Mac OS machines\n","import os\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","\n","#raw data in dictionary form x01, x02, x03, y3\n","my_dict = {\n","    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n","    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n","    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n","    'y3' : [30.99, 32.2, 31.7, 30.92, 32.4, 31.4, 35.53, 46.4]\n","}\n","#normalized inputs in array\n","xdata = []\n","xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]]\n","xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n","xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0])\n","xdata.append([20./20.2, 14.5/14.5, 308.0/308.0])\n","xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0])\n","xdata.append([24./20.2, 13.0/14.5, 310.8/308.0])\n","xdata.append([36./20.2, 14.5/14.5, 308.0/308.0])\n","\n","#data frame\n","df = pd.DataFrame(my_dict)\n","#devide by the median to normalize\n","df.x01= df.x01/20.2\n","df.x02= df.x02/14.5\n","df.x03= df.x03/308.0\n","#normalize output array\n","df.y3= df.y3/32.401\n","df.head\n","print (df.x01, df.x02, df.x03, df.y3)\n","\n","xarray= np.array(xdata)\n","print (xdata)\n","print (xarray)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFe7Mp64kQAb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697511671171,"user_tz":420,"elapsed":1097,"user":{"displayName":"Mason Rodriguez Rand","userId":"15413783060171038202"}},"outputId":"020fb0e2-f204-445a-8ad5-b59e15416f95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_one (Dense)           (None, 1)                 4         \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["                                                                 \n"," dense_two (Dense)           (None, 1)                 2         \n","                                                                 \n"," dense_three (Dense)         (None, 1)                 2         \n","                                                                 \n","=================================================================\n","Total params: 8 (32.00 Byte)\n","Trainable params: 8 (32.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","(3, 1)\n","(1, 1)\n","(1, 1)\n"]}],"source":["# define model\n","\n","#As seen below, we have created three dense layers each with just one neuron.\n","#A dense layer is a layer in neural network that’s fully connected.\n","#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n","#In the first layer, we need to provide the input shape, which is 3 in this case.\n","#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n","\n","from keras import backend as K\n","#initialize weights with values between -0.2 and 1.2\n","initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n","\n","# define three layer model with one neuron in each layer\n","model = keras.Sequential([\n","    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n","    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n","    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n","  ])\n","model.summary()\n","\n","#set starting values to those used in first principles model\n","w01n =  1.23\n","w02n =  0.40\n","w03n =  0.70\n","b1n =  -0.15\n","w12n =  0.72\n","b2n =  -0.12\n","w23n =  0.7\n","b3n =  0.01\n","\n","weights0 =  [[ w01n], [w02n], [ w03n]]\n","w0array= np.array(weights0)\n","print(np.shape(w0array))\n","bias0 = [b1n]\n","bias0array= np.array(bias0)\n","L0=[]\n","L0.append(w0array)\n","L0.append(bias0array)\n","model.layers[0].set_weights(L0)\n","\n","weights1 =  [[ w12n]]\n","w1array= np.array(weights1)\n","print(np.shape(w1array))\n","bias1 = [b2n]\n","bias1array= np.array(bias1)\n","L1=[]\n","L1.append(w1array)\n","L1.append(bias1array)\n","model.layers[1].set_weights(L1)\n","\n","weights2 =  [[ w23n]]\n","w2array= np.array(weights2)\n","print(np.shape(w2array))\n","bias2 = [b3n]\n","bias2array= np.array(bias2)\n","L2=[]\n","L2.append(w2array)\n","L2.append(bias2array)\n","model.layers[2].set_weights(L2)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVch6MrOkQAb"},"outputs":[],"source":["#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation.\n","#It’s one of the most popular gradient descent optimization algorithms for deep learning networks.\n","#RMSprop is an optimizer that’s reliable and fast.\n","#We’re compiling the mode using the model.compile function. The loss function used here\n","#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n","\n","#Running model.fit successive times extends the calculation to addtional epochs.\n","\n","rms = keras.optimizers.RMSprop(0.0015)\n","model.compile(loss='mean_absolute_error',optimizer=rms)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"krKYqSdHkQAc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697512110200,"user_tz":420,"elapsed":7547,"user":{"displayName":"Mason Rodriguez Rand","userId":"15413783060171038202"}},"outputId":"7892d632-04d1-4ec0-b698-5e767296de63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/400\n","1/1 [==============================] - 0s 468ms/step - loss: 0.0131\n","Epoch 2/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0241\n","Epoch 3/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0130\n","Epoch 4/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0155\n","Epoch 5/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0147\n","Epoch 6/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0127\n","Epoch 7/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0159\n","Epoch 8/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0144\n","Epoch 9/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0127\n","Epoch 10/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0145\n","Epoch 11/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0127\n","Epoch 12/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0146\n","Epoch 13/400\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0126\n","Epoch 14/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0147\n","Epoch 15/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0126\n","Epoch 16/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0128\n","Epoch 17/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0147\n","Epoch 18/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0128\n","Epoch 19/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0148\n","Epoch 20/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0127\n","Epoch 21/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0149\n","Epoch 22/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 23/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0150\n","Epoch 24/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 25/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0128\n","Epoch 26/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0150\n","Epoch 27/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0127\n","Epoch 28/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0151\n","Epoch 29/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 30/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0152\n","Epoch 31/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 32/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0128\n","Epoch 33/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0170\n","Epoch 34/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0145\n","Epoch 35/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0132\n","Epoch 36/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0164\n","Epoch 37/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0141\n","Epoch 38/400\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0137\n","Epoch 39/400\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0159\n","Epoch 40/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0138\n","Epoch 41/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0141\n","Epoch 42/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0156\n","Epoch 43/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0135\n","Epoch 44/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0146\n","Epoch 45/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0152\n","Epoch 46/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0132\n","Epoch 47/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 48/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0149\n","Epoch 49/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0129\n","Epoch 50/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0155\n","Epoch 51/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n","Epoch 52/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 53/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0160\n","Epoch 54/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0143\n","Epoch 55/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0127\n","Epoch 56/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0145\n","Epoch 57/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0127\n","Epoch 58/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0146\n","Epoch 59/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 60/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0147\n","Epoch 61/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0126\n","Epoch 62/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0148\n","Epoch 63/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n","Epoch 64/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0128\n","Epoch 65/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 66/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0127\n","Epoch 67/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0149\n","Epoch 68/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0126\n","Epoch 69/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 70/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 71/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0129\n","Epoch 72/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0168\n","Epoch 73/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0144\n","Epoch 74/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0133\n","Epoch 75/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0163\n","Epoch 76/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0140\n","Epoch 77/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0137\n","Epoch 78/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0158\n","Epoch 79/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0137\n","Epoch 80/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0142\n","Epoch 81/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0155\n","Epoch 82/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0134\n","Epoch 83/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0147\n","Epoch 84/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0152\n","Epoch 85/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0131\n","Epoch 86/400\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0151\n","Epoch 87/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0148\n","Epoch 88/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0129\n","Epoch 89/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0156\n","Epoch 90/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0146\n","Epoch 91/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 92/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0161\n","Epoch 93/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0143\n","Epoch 94/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0128\n","Epoch 95/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0144\n","Epoch 96/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0127\n","Epoch 97/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0145\n","Epoch 98/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0127\n","Epoch 99/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0146\n","Epoch 100/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0126\n","Epoch 101/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0148\n","Epoch 102/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0126\n","Epoch 103/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0149\n","Epoch 104/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0125\n","Epoch 105/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0169\n","Epoch 106/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0156\n","Epoch 107/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0136\n","Epoch 108/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0140\n","Epoch 109/400\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0153\n","Epoch 110/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0134\n","Epoch 111/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0145\n","Epoch 112/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0151\n","Epoch 113/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0131\n","Epoch 114/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 115/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0148\n","Epoch 116/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0129\n","Epoch 117/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0154\n","Epoch 118/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0146\n","Epoch 119/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 120/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0159\n","Epoch 121/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0143\n","Epoch 122/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0127\n","Epoch 123/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0144\n","Epoch 124/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0127\n","Epoch 125/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0145\n","Epoch 126/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 127/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0147\n","Epoch 128/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n","Epoch 129/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 130/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n","Epoch 131/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0129\n","Epoch 132/400\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0163\n","Epoch 133/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0147\n","Epoch 134/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n","Epoch 135/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0128\n","Epoch 136/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0164\n","Epoch 137/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0142\n","Epoch 138/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0132\n","Epoch 139/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0160\n","Epoch 140/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0139\n","Epoch 141/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0137\n","Epoch 142/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0157\n","Epoch 143/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0136\n","Epoch 144/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0142\n","Epoch 145/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0154\n","Epoch 146/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0134\n","Epoch 147/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0147\n","Epoch 148/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0151\n","Epoch 149/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0131\n","Epoch 150/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0151\n","Epoch 151/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 152/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0128\n","Epoch 153/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0156\n","Epoch 154/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0145\n","Epoch 155/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 156/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0129\n","Epoch 157/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0157\n","Epoch 158/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0146\n","Epoch 159/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 160/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0162\n","Epoch 161/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0143\n","Epoch 162/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0127\n","Epoch 163/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0144\n","Epoch 164/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0127\n","Epoch 165/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0145\n","Epoch 166/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0127\n","Epoch 167/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0147\n","Epoch 168/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 169/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 170/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n","Epoch 171/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0149\n","Epoch 172/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0125\n","Epoch 173/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0170\n","Epoch 174/400\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0156\n","Epoch 175/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0136\n","Epoch 176/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0140\n","Epoch 177/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0153\n","Epoch 178/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0134\n","Epoch 179/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0145\n","Epoch 180/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0151\n","Epoch 181/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0131\n","Epoch 182/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0150\n","Epoch 183/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0148\n","Epoch 184/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0129\n","Epoch 185/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0155\n","Epoch 186/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0145\n","Epoch 187/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0126\n","Epoch 188/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0159\n","Epoch 189/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0143\n","Epoch 190/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0127\n","Epoch 191/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0144\n","Epoch 192/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0126\n","Epoch 193/400\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0145\n","Epoch 194/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0126\n","Epoch 195/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0146\n","Epoch 196/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n","Epoch 197/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0148\n","Epoch 198/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n","Epoch 199/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0167\n","Epoch 200/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0155\n","Epoch 201/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0136\n","Epoch 202/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0140\n","Epoch 203/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0153\n","Epoch 204/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0134\n","Epoch 205/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0144\n","Epoch 206/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0150\n","Epoch 207/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0131\n","Epoch 208/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0149\n","Epoch 209/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0148\n","Epoch 210/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0129\n","Epoch 211/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0154\n","Epoch 212/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0145\n","Epoch 213/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 214/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0159\n","Epoch 215/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0143\n","Epoch 216/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 217/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0144\n","Epoch 218/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0126\n","Epoch 219/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0145\n","Epoch 220/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 221/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0147\n","Epoch 222/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0125\n","Epoch 223/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0128\n","Epoch 224/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0162\n","Epoch 225/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n","Epoch 226/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0125\n","Epoch 227/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 228/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0125\n","Epoch 229/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0167\n","Epoch 230/400\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0155\n","Epoch 231/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0136\n","Epoch 232/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0139\n","Epoch 233/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0153\n","Epoch 234/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0134\n","Epoch 235/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0144\n","Epoch 236/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 237/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0131\n","Epoch 238/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0149\n","Epoch 239/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 240/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0129\n","Epoch 241/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0154\n","Epoch 242/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0146\n","Epoch 243/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 244/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0159\n","Epoch 245/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0143\n","Epoch 246/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 247/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0144\n","Epoch 248/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n","Epoch 249/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0145\n","Epoch 250/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0125\n","Epoch 251/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0147\n","Epoch 252/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n","Epoch 253/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0128\n","Epoch 254/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0162\n","Epoch 255/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0147\n","Epoch 256/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0125\n","Epoch 257/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0128\n","Epoch 258/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0163\n","Epoch 259/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0142\n","Epoch 260/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0133\n","Epoch 261/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0160\n","Epoch 262/400\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0139\n","Epoch 263/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0137\n","Epoch 264/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0156\n","Epoch 265/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0136\n","Epoch 266/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0142\n","Epoch 267/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0153\n","Epoch 268/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0133\n","Epoch 269/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0147\n","Epoch 270/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0150\n","Epoch 271/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0131\n","Epoch 272/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0151\n","Epoch 273/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0148\n","Epoch 274/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0128\n","Epoch 275/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0156\n","Epoch 276/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0145\n","Epoch 277/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 278/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0128\n","Epoch 279/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0157\n","Epoch 280/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n","Epoch 281/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n","Epoch 282/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0162\n","Epoch 283/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0143\n","Epoch 284/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0127\n","Epoch 285/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0144\n","Epoch 286/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0127\n","Epoch 287/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0145\n","Epoch 288/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 289/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n","Epoch 290/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 291/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0147\n","Epoch 292/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0125\n","Epoch 293/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0149\n","Epoch 294/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n","Epoch 295/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0170\n","Epoch 296/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0156\n","Epoch 297/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0136\n","Epoch 298/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0140\n","Epoch 299/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0153\n","Epoch 300/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0133\n","Epoch 301/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0145\n","Epoch 302/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 303/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0131\n","Epoch 304/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0150\n","Epoch 305/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 306/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0128\n","Epoch 307/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0155\n","Epoch 308/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0145\n","Epoch 309/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 310/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0160\n","Epoch 311/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0143\n","Epoch 312/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 313/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0144\n","Epoch 314/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n","Epoch 315/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0145\n","Epoch 316/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0126\n","Epoch 317/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n","Epoch 318/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n","Epoch 319/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0128\n","Epoch 320/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0163\n","Epoch 321/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n","Epoch 322/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n","Epoch 323/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0147\n","Epoch 324/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n","Epoch 325/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0168\n","Epoch 326/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0155\n","Epoch 327/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0136\n","Epoch 328/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0140\n","Epoch 329/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0153\n","Epoch 330/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0133\n","Epoch 331/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0145\n","Epoch 332/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 333/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0131\n","Epoch 334/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 335/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 336/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0128\n","Epoch 337/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0154\n","Epoch 338/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0145\n","Epoch 339/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0126\n","Epoch 340/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0159\n","Epoch 341/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0143\n","Epoch 342/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n","Epoch 343/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0144\n","Epoch 344/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0126\n","Epoch 345/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0145\n","Epoch 346/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0125\n","Epoch 347/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0146\n","Epoch 348/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n","Epoch 349/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0128\n","Epoch 350/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0162\n","Epoch 351/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0146\n","Epoch 352/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n","Epoch 353/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0147\n","Epoch 354/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0125\n","Epoch 355/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0168\n","Epoch 356/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0155\n","Epoch 357/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0136\n","Epoch 358/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0140\n","Epoch 359/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0153\n","Epoch 360/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0133\n","Epoch 361/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0145\n","Epoch 362/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0150\n","Epoch 363/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0131\n","Epoch 364/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0149\n","Epoch 365/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0148\n","Epoch 366/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0128\n","Epoch 367/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0154\n","Epoch 368/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0145\n","Epoch 369/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0126\n","Epoch 370/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0159\n","Epoch 371/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0143\n","Epoch 372/400\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0126\n","Epoch 373/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0144\n","Epoch 374/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0125\n","Epoch 375/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0145\n","Epoch 376/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0125\n","Epoch 377/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0146\n","Epoch 378/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n","Epoch 379/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0166\n","Epoch 380/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0142\n","Epoch 381/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0130\n","Epoch 382/400\n","1/1 [==============================] - 0s 8ms/step - loss: 0.0160\n","Epoch 383/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0140\n","Epoch 384/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0134\n","Epoch 385/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0157\n","Epoch 386/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0137\n","Epoch 387/400\n","1/1 [==============================] - 0s 7ms/step - loss: 0.0139\n","Epoch 388/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0154\n","Epoch 389/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0134\n","Epoch 390/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0144\n","Epoch 391/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0151\n","Epoch 392/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0132\n","Epoch 393/400\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0149\n","Epoch 394/400\n","1/1 [==============================] - 0s 6ms/step - loss: 0.0149\n","Epoch 395/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0129\n","Epoch 396/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0153\n","Epoch 397/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0146\n","Epoch 398/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0127\n","Epoch 399/400\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0158\n","Epoch 400/400\n","1/1 [==============================] - 0s 5ms/step - loss: 0.0143\n","best epoch =  378\n","smallest loss = 0.012462273240089417\n"]}],"source":["#After the compilation of the model, we’ll use the fit method with 500 epochs.\n","#I started with epochs value of 100 and then tested the model after training.\n","#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again.\n","#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs\n","#I found acceptable prediction accuracy.\n","\n","#The fit method takes three parameters; namely, x, y, and number of epochs.\n","#During model training, if all the batches of data are seen by the model once,\n","#we say that one epoch has been completed.\n","\n","# Add an early stopping callback\n","es = tf.keras.callbacks.EarlyStopping(\n","    monitor='loss',\n","    mode='min',\n","    patience = 80,\n","    restore_best_weights = True,\n","    verbose=1)\n","# Add a checkpoint where loss is minimum, and save that model\n","mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss',\n","                     mode='min',  verbose=1, save_best_only=True)\n","\n","historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n","\n","loss_hist = historyData.history['loss']\n","#The above line will return a dictionary, access it's info like this:\n","best_epoch = np.argmin(historyData.history['loss']) + 1\n","print ('best epoch = ', best_epoch)\n","print('smallest loss =', np.min(loss_hist))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gy0SSscmkQAc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697512116203,"user_tz":420,"elapsed":1318,"user":{"displayName":"Mason Rodriguez Rand","userId":"15413783060171038202"}},"outputId":"528e21bd-5032-4f7f-b2dc-bb0c981e7130"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.2096403 ]\n"," [0.36096197]\n"," [0.7128528 ]]\n","w01 =  1.2096403 w02 =  0.36096197 w03 =  0.7128528\n","[-0.14252366]\n","b1 =  [-0.14252366]\n","[[0.70485824]]\n","w12 =  0.70485824\n","[-0.10959321]\n","b2 =  [-0.10959321]\n","[[0.6814442]]\n","w23 =  0.6814442\n","[0.02343724]\n","b3 =  [0.02343724]\n","x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n","1/1 [==============================] - 0s 69ms/step\n","0.9900990099009901 0.896551724137931 1.009090909090909 0.9564519613592172 [[0.95651555]]\n","1/1 [==============================] - 0s 27ms/step\n","0.9900990099009901 1.0 1.0 0.9937964877627233 [[0.9713384]]\n","1/1 [==============================] - 0s 25ms/step\n","0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9783648652819357 [[0.9786807]]\n","1/1 [==============================] - 0s 24ms/step\n","1.0 0.896551724137931 1.009090909090909 0.954291534211907 [[0.96226805]]\n","1/1 [==============================] - 0s 21ms/step\n","0.9900990099009901 1.0 1.0 0.9999691367550383 [[0.9713384]]\n","1/1 [==============================] - 0s 28ms/step\n","1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.9844334]]\n","1/1 [==============================] - 0s 23ms/step\n","1.188118811881188 0.896551724137931 1.009090909090909 1.096571093484769 [[1.0715683]]\n","1/1 [==============================] - 0s 23ms/step\n","1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4315493]]\n","  \n","x01,  x02,   x03,  y3,  a3*32.4:\n","1/1 [==============================] - 0s 24ms/step\n","20.0 13.0 310.8 30.989043548038634 [[30.991106]]\n","1/1 [==============================] - 0s 30ms/step\n","20.0 14.5 308.0 32.19900620351223 [[31.471365]]\n","1/1 [==============================] - 0s 20ms/step\n","20.0 15.3 306.0 31.699021635134713 [[31.709255]]\n","1/1 [==============================] - 0s 28ms/step\n","20.2 13.0 310.8 30.919045708465788 [[31.177486]]\n","1/1 [==============================] - 0s 26ms/step\n","20.0 14.5 308.0 32.39900003086324 [[31.471365]]\n","1/1 [==============================] - 0s 22ms/step\n","20.2 15.3 306.0 31.3990308941082 [[31.895643]]\n","1/1 [==============================] - 0s 26ms/step\n","23.999999999999996 13.0 310.8 35.52890342890652 [[34.718815]]\n","1/1 [==============================] - 0s 27ms/step\n","36.0 14.5 308.0 46.398567945433776 [[46.3822]]\n"]}],"source":["from __future__ import print_function\n","#For results of training network:\n","\n","#keras.layer.get_weights() function retrieves weight values\n","first_layer_weights = model.layers[0].get_weights()[0]\n","w01 = first_layer_weights[0][0]\n","w02 = first_layer_weights[1][0]\n","w03 = first_layer_weights[2][0]\n","first_layer_bias  = model.layers[0].get_weights()[1]\n","b1 = first_layer_bias\n","second_layer_weights = model.layers[1].get_weights()[0]\n","w12 = second_layer_weights[0][0]\n","second_layer_bias  = model.layers[1].get_weights()[1]\n","b2 = second_layer_bias\n","third_layer_weights = model.layers[2].get_weights()[0]\n","w23 = third_layer_weights[0][0]\n","third_layer_bias  = model.layers[2].get_weights()[1]\n","b3 = third_layer_bias\n","\n","#print weights and biases\n","print (first_layer_weights)\n","print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n","print (first_layer_bias)\n","print ('b1 = ', b1)\n","print (second_layer_weights)\n","print ('w12 = ', w12)\n","print (second_layer_bias)\n","print ('b2 = ', b2)\n","print (third_layer_weights)\n","print ('w23 = ', w23)\n","print (third_layer_bias)\n","print ('b3 = ', b3)\n","\n","#use model.predict() function to print model predictions for data conditions\n","xarray= np.array(xdata)\n","print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n","test = []\n","for i in range(0,8):\n","    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n","    testarray = np.array(test)\n","    a3 = model.predict(testarray)\n","    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n","print('  ')\n","print ('x01,  x02,   x03,  y3,  a3*32.4:')\n","for i in range(0,8):\n","    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n","    testarray = np.array(test)\n","    a3 = model.predict(testarray)\n","    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6-3HlQOkQAc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMRmmU1KkQAc"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["w01 =  1.2096403 w02 =  0.36096197 w03 =  0.7128528\n","[-0.14252366]\n","b1 =  [-0.14252366]\n","[[0.70485824]]\n","w12 =  0.70485824\n","[-0.10959321]\n","b2 =  [-0.10959321]\n","[[0.6814442]]\n","w23 =  0.6814442\n","[0.02343724]\n","b3 =  [0.02343724]\n","\n","\n","x01,  x02,   x03,  y3,  a3*32.4:\n","36.0 14.5 308.0 46.398567945433776 [[46.3822]]"],"metadata":{"id":"OSKZYmQCnFAc"}},{"cell_type":"code","source":[],"metadata":{"id":"jGIeCvwenN_9"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"provenance":[{"file_id":"1rlBHTFfsQbfPACgC2VhHorDTGFOP9GUm","timestamp":1697511608697}]}},"nbformat":4,"nbformat_minor":0}